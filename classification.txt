epoch, training loss, validation loss
0  0.787406  0.785332
1  0.253617  0.217911
2  0.222175  0.195392
3  0.174232  0.159987
4  0.140729  0.117779
5  0.110537  0.071414
6  0.077143  0.069788
7  0.072068  0.064881
8  0.067543  0.064025
9  0.065118  0.062023
10  0.061659  0.056716
11  0.055464  0.049869
12  0.054569  0.048798
13  0.052590  0.049056
14  0.050618  0.048668
15  0.050140  0.047448
16  0.049577  0.051034
17  0.049057  0.053354
18  0.046904  0.048710
19  0.046552  0.054068
20  0.046531  0.051783
21  0.042429  0.046232
22  0.040572  0.047847
23  0.039526  0.047067
24  0.039561  0.045989
25  0.039140  0.049006
26  0.038555  0.049173
27  0.038127  0.049176
28  0.037643  0.045676
29  0.037444  0.047226
30  0.036177  0.050360
31  0.034076  0.047836
32  0.033769  0.045985
33  0.033290  0.045755
34  0.032967  0.047596
35  0.033288  0.048316
36  0.032667  0.048288
37  0.032704  0.047798
38  0.031988  0.047943
39  0.031471  0.047892
40  0.031688  0.048301
41  0.030031  0.049672
42  0.029711  0.048631
43  0.029543  0.051071
44  0.029303  0.049591
45  0.029337  0.049194
46  0.029117  0.047032
47  0.028870  0.049002
48  0.028535  0.048271
49  0.028461  0.049737
50  0.028337  0.049428
51  0.027588  0.049818
52  0.027661  0.050905
53  0.027628  0.049664
54  0.027510  0.051676
55  0.027197  0.051620
56  0.027278  0.051206
57  0.027294  0.050201
58  0.027224  0.051086
59  0.027022  0.050537
60  0.026978  0.050950
